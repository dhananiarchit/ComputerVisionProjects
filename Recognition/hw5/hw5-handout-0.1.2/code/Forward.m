function [output, act_h, act_a] = Forward(W, b, X)
% [OUT, act_h, act_a] = Forward(W, b, X) performs forward propogation on the
% input data 'X' uisng the network defined by weights and biases 'W' and 'b'
% (as generated by InitializeNetwork(..)).
%
% This function should return the final softmax output layer activations in OUT,
% as well as the hidden layer post activations in 'act_h', and the hidden layer
% pre activations in 'act_a'.
act_a = {};
act_h = {};

for i = 1:length(b)
    if(i==1)
        act_a{i} = W{i}*X + b{i};
    else
        act_a{i} = W{i}*act_h{i-1} + b{i};
    end
    act_h{i} = sigmoid(act_a{i});
end
output = exp(-act_a{end});
output = output./sum(output);